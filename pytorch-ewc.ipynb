{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Guidelines of Sample Code (Pytorch)\n",
    "\n",
    "    See the annotations at every markdown blocks correspoding to each code blocks, and also # TODO annotations. :D\n",
    "\n",
    "# Usage guideline of Jupyter Notebook (If needed)\n",
    "\n",
    "    Installation   : https://jupyter.org/install  \n",
    "    User Document  : https://jupyter-notebook.readthedocs.io/en/latest/user-documentation.html\n",
    "\n",
    "# Test Environment (Recommended)\n",
    "\n",
    "    In test time, we will evaluate the given codes from you with the following version of libraries.  \n",
    "    So, it is highly recommended to use those packages with specific version below.\n",
    "\n",
    "    test environment : pytorch\n",
    "\n",
    "### Packages\n",
    "    python   : 3.8.17  \n",
    "    torch    : 2.0.1   \n",
    "    skimage  : 0.21.0  \n",
    "    cv2      : 4.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries (Do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset (Do not change!)\n",
    "\n",
    "### Notice 1\n",
    "    This function do split your dataset of 1000 classes into 10 groups of 100 each.    \n",
    "    So, it is needed to be implemented just once at first to split your dataset for continual learning.   \n",
    "    *Again, you dont need to use this function in every tranining time if you already split your dataset into 10 groups.\n",
    "\n",
    "    Notice the annotation codes below. (You can see this codes in 'main' block.)\n",
    "\n",
    "```python\n",
    "        parser = argparse.ArgumentParser()   \n",
    "        # Change this as 'False' after dividing your datsaet into 10 groups.\n",
    "        parser.add_argument('--div_data',   default = True)  \n",
    "        args = parser.parse_args(args=[])  \n",
    "```\n",
    "\n",
    "### Notice 2\n",
    "    We reshapes all the input data size into constant 128x128.   \n",
    "    Until further notification, use this constant size. \n",
    "\n",
    "```python\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 130):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))  # resize image into 128 x 128 \n",
    "                x_train.append(img)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(validation_num):\n",
    "    # TODO : set dataset path\n",
    "    # TODO : We recommends you to place your code and tranining dataset in the same location.\n",
    "    \n",
    "    dir = './Koh_Young_AI_data/'\n",
    "    \n",
    "\n",
    "    for div_idx in range(0, 10): # Div into 10 groups\n",
    "        # Divide data 0-129 for training, 130-150 for validation.\n",
    "        x_train = []\n",
    "        x_valid = []\n",
    "        y_train = []\n",
    "        y_valid = []\n",
    "        start   = 100*div_idx + 1\n",
    "        end     = 100*div_idx + 101\n",
    "\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_train.append(img)\n",
    "\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_valid.append(img)\n",
    "\n",
    "        # Split corresponding output label data.\n",
    "        for folder_idx in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                y_train.append(np.array([folder_idx]))\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                y_valid.append(np.array([folder_idx]))\n",
    "\n",
    "        # Convert list to numpy \n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_valid = np.array(x_valid)\n",
    "        y_valid = np.array(y_valid)\n",
    "\n",
    "        # TODO : Define train data and valid data directory path.\n",
    "        # TODO : Recommends not to change these directory paths. \n",
    "        train_save_dir = 'train_data'\n",
    "        valid_save_dir = 'valid_data'\n",
    "        if not os.path.exists(train_save_dir):\n",
    "            os.makedirs(train_save_dir)\n",
    "\n",
    "        if not os.path.exists(valid_save_dir):\n",
    "            os.makedirs(valid_save_dir)\n",
    "\n",
    "        # TODO : Save train/valid data\n",
    "        np.save(f'./train_data/x_data_{div_idx+1}', x_train)\n",
    "        np.save(f'./train_data/y_data_{div_idx+1}', y_train)\n",
    "        np.save(f'./valid_data/x_data_{div_idx+1}', x_valid)\n",
    "        np.save(f'./valid_data/y_data_{div_idx+1}', y_valid)\n",
    "\n",
    "        print(f\" ===================== Done in {div_idx} ===================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader (Do not change!)\n",
    "\n",
    "    You can define your own dataloader with API of torch.utils.data.Dataset.  \n",
    "    This can usually help you to reduce computational burden when dealing with high dimensional data, such as images.  \n",
    "\n",
    "    reference url : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_data, y_data, device):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # .transpose(0, 2) : width x height x channel (0, 1, 2) ---> channel x width x height (2, 0, 1).\n",
    "        # .squeeze(0) : add extra dimension at axis 0.\n",
    "        x = torch.FloatTensor(self.x_data[idx]).transpose(0, 2)\n",
    "        y = torch.LongTensor(self.y_data[idx]).squeeze(0)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "def load_train_data(class_num):\n",
    "    # TODO : set 'class_path' with your train_data path.\n",
    "    class_path  = f'./train_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "    return x_data, y_data\n",
    "\n",
    "def load_valid_data(class_num):\n",
    "    # TODO : set 'class_path' with your valid_data path.\n",
    "    class_path  = f'./valid_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "\n",
    "    # return processed data. \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tranining function (You can modify this part!)\n",
    "\n",
    "    Set your model with train mode as 'model.train()'.   \n",
    "\n",
    "    useful reference : https://wikidocs.net/195118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, optimizer, num_epochs, train_data_loader, criterion,train_dataset,indexs):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_train           : input data for tranining\n",
    "    optimizer         : optimizer\n",
    "    num_epoches       : number of iteration\n",
    "    train_data_loader : dataloder of training dataset\n",
    "    indexs            : train seq\n",
    "    \"\"\"\n",
    "    \n",
    "    if indexs== 0:\n",
    "        num_epochs=20\n",
    "    else:\n",
    "        num_epochs=10\n",
    "        \n",
    "    ewc.model.train()                     # Set train mode. \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        acc      = 0 # Accuracy\n",
    "        avg_cost = 0 # Average Cost \n",
    "        for x, y in train_data_loader:\n",
    "            loss,out=ewc.forward_backward_update(x.to(device), y.to(device))\n",
    "            #out = ewc.model(x.to(device))            # Inference (batch,classes)\n",
    "            #out = model(x.to(device))\n",
    "            _, preds = torch.max(out, 1)         # preds : Predicted class\n",
    "            #cost = criterion(out, y.to(device))  # Calculates errors between true label and predictions with respect to your criterion. \n",
    "\n",
    "            # Optimize processs.\n",
    "            #optimizer.zero_grad()\n",
    "            #cost.backward()\n",
    "            #optimizer.step()\n",
    "\n",
    "            avg_cost += loss # Average cost \n",
    "            acc      += torch.sum(preds.detach().cpu() == (y.data).detach().cpu()) # Accuracy\n",
    "        print(f\" # - EPOCHS {epoch + 1} / {num_epochs} | AvgCost {avg_cost} | Accuracy : {acc/len(x_train)} - #\")\n",
    "    ewc.register_ewc_params(train_dataset, 32, 20)\n",
    "    #pretrained_dict = ewc.model.state_dict()\n",
    "    #print(pretrained_dict)\n",
    "    #exit(1)\n",
    "    # Return trainded model and accuracy. \n",
    "    return ewc.model, acc/len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define validataion function (Do not change!)\n",
    "\n",
    "    And eval mode as 'model.eval()' or 'model.train(False)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, x_valid, valid_data_loader, criterion):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_vallid          : input data for validation\n",
    "    valid_data_loader : dataloder of valid dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set eval mode\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    for x, y in valid_data_loader:\n",
    "        out = model(x.data.to(device))\n",
    "        _, preds = torch.max(out, 1)\n",
    "        cost  = criterion(out, y.to(device))\n",
    "        acc += torch.sum(preds.detach().cpu() == (y.data).detach().cpu())\n",
    "    print(f\" # - ValidCost {cost} | Accuracy : {acc / len(x_valid)} - #\")\n",
    "\n",
    "    # Return Accuracy \n",
    "    return acc/len(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model and hyperparameter (You can modify this part!)\n",
    "\n",
    "    Here is the pivotal part of your competition.\n",
    "    We gives a simple CNN model, for example. \n",
    "    Go make your own model!         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticWeightConsolidation:\n",
    "\n",
    "    def __init__(self, model, crit, lr=0.001, weight=40000):\n",
    "        self.model = model\n",
    "        self.weight = weight\n",
    "        self.crit = crit\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "        #self.make_fisher()\n",
    "        \n",
    "    def _update_mean_params(self):\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            _buff_param_name = param_name.replace('.', '__')\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())\n",
    "\n",
    "    def _update_fisher_params(self, current_ds, batch_size, num_batch):\n",
    "        dl = DataLoader(current_ds, batch_size, shuffle=True)\n",
    "        log_liklihoods = []\n",
    "        for i, (inputs, target) in enumerate(dl):\n",
    "            if i > num_batch:\n",
    "                break\n",
    "            output = torch.nn.functional.log_softmax(self.model(inputs.to(device)), dim=1)\n",
    "            log_liklihoods.append(output[:, target])\n",
    "        log_likelihood = torch.cat(log_liklihoods).mean()\n",
    "        grad_log_liklihood = torch.autograd.grad(log_likelihood, self.model.parameters())\n",
    "        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "        for _buff_param_name, param in zip(_buff_param_names, grad_log_liklihood):\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_fisher', param.data.clone() ** 2)\n",
    "\n",
    "    def register_ewc_params(self, dataset, batch_size, num_batches):\n",
    "        self._update_fisher_params(dataset, batch_size, num_batches)\n",
    "        self._update_mean_params()\n",
    "\n",
    "    def _compute_consolidation_loss(self, weight):\n",
    "        try:\n",
    "            losses = []\n",
    "            for param_name, param in self.model.named_parameters():\n",
    "                _buff_param_name = param_name.replace('.', '__')\n",
    "                estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name))\n",
    "                estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name))\n",
    "                losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())\n",
    "            return weight * sum(losses)\n",
    "        except AttributeError:\n",
    "            return 0\n",
    "\n",
    "    def forward_backward_update(self, inputs, target):\n",
    "        output = self.model(inputs)\n",
    "        loss = self._compute_consolidation_loss(self.weight) + self.crit(output, target.to(device))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss,output\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.model, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=1000, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=32\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 32, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 64, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 128, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 256, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3, 8, 6, 6])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = resnet50().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# TODO : Define your model\n",
    "batch_size        = 16\n",
    "learning_rate     = 0.001\n",
    "num_epochs        = 15\n",
    "optimizer         = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "random_seed       = 555\n",
    "validation_num    = 20 # for 150 images for class, the number for validation data\n",
    "criterion = nn.CrossEntropyLoss() # Define criterion. \n",
    "\n",
    "ewc = ElasticWeightConsolidation(model, crit=criterion, lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning. (Do not change!)\n",
    "\n",
    "### WARNING:\n",
    "    The training and validation datasets each SHOULD BE prepared properly beforehand.  \n",
    "    If not, the submitted code from you will be immediately rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # - EPOCHS 1 / 20 | AvgCost 3151.29833984375 | Accuracy : 0.04685314744710922 - #\n",
      " # - EPOCHS 2 / 20 | AvgCost 2304.6611328125 | Accuracy : 0.12525252997875214 - #\n",
      " # - EPOCHS 3 / 20 | AvgCost 1961.093505859375 | Accuracy : 0.19316239655017853 - #\n",
      " # - EPOCHS 4 / 20 | AvgCost 1701.47802734375 | Accuracy : 0.2777000665664673 - #\n",
      " # - EPOCHS 5 / 20 | AvgCost 1443.014404296875 | Accuracy : 0.37668997049331665 - #\n",
      " # - EPOCHS 6 / 20 | AvgCost 1223.1766357421875 | Accuracy : 0.4668997526168823 - #\n",
      " # - EPOCHS 7 / 20 | AvgCost 1005.3316650390625 | Accuracy : 0.556332528591156 - #\n",
      " # - EPOCHS 8 / 20 | AvgCost 790.9276123046875 | Accuracy : 0.6406371593475342 - #\n",
      " # - EPOCHS 9 / 20 | AvgCost 603.7872924804688 | Accuracy : 0.7193472981452942 - #\n",
      " # - EPOCHS 10 / 20 | AvgCost 463.1643981933594 | Accuracy : 0.7867909669876099 - #\n",
      " # - EPOCHS 11 / 20 | AvgCost 363.6328430175781 | Accuracy : 0.8275835514068604 - #\n",
      " # - EPOCHS 12 / 20 | AvgCost 282.5030822753906 | Accuracy : 0.8677544593811035 - #\n",
      " # - EPOCHS 13 / 20 | AvgCost 228.69363403320312 | Accuracy : 0.8929293155670166 - #\n",
      " # - EPOCHS 14 / 20 | AvgCost 184.6468963623047 | Accuracy : 0.9140636920928955 - #\n",
      " # - EPOCHS 15 / 20 | AvgCost 144.3822021484375 | Accuracy : 0.9337995052337646 - #\n",
      " # - EPOCHS 16 / 20 | AvgCost 127.94042205810547 | Accuracy : 0.942501962184906 - #\n",
      " # - EPOCHS 17 / 20 | AvgCost 106.45751953125 | Accuracy : 0.9529914259910583 - #\n",
      " # - EPOCHS 18 / 20 | AvgCost 81.79967498779297 | Accuracy : 0.9655011892318726 - #\n",
      " # - EPOCHS 19 / 20 | AvgCost 83.17900848388672 | Accuracy : 0.9627816677093506 - #\n",
      " # - EPOCHS 20 / 20 | AvgCost 54.764583587646484 | Accuracy : 0.978010892868042 - #\n",
      " # - ValidCost 0.004437668714672327 | Accuracy : 0.9565656781196594 - #\n",
      "0 Iteration Done.\n",
      " # - EPOCHS 1 / 10 | AvgCost 2690.80859375 | Accuracy : 0.3121989071369171 - #\n",
      " # - EPOCHS 2 / 10 | AvgCost 1283.065185546875 | Accuracy : 0.6602952480316162 - #\n",
      " # - EPOCHS 3 / 10 | AvgCost 925.5817260742188 | Accuracy : 0.7977467179298401 - #\n",
      " # - EPOCHS 4 / 10 | AvgCost 755.2998046875 | Accuracy : 0.8550116419792175 - #\n",
      " # - EPOCHS 5 / 10 | AvgCost 655.484375 | Accuracy : 0.8884226679801941 - #\n",
      " # - EPOCHS 6 / 10 | AvgCost 601.5501098632812 | Accuracy : 0.9015539884567261 - #\n",
      " # - EPOCHS 7 / 10 | AvgCost 563.7235717773438 | Accuracy : 0.9152292013168335 - #\n",
      " # - EPOCHS 8 / 10 | AvgCost 545.2293701171875 | Accuracy : 0.9195027351379395 - #\n",
      " # - EPOCHS 9 / 10 | AvgCost 506.92633056640625 | Accuracy : 0.9299922585487366 - #\n",
      " # - EPOCHS 10 / 10 | AvgCost 453.1275634765625 | Accuracy : 0.9418026208877563 - #\n",
      " # - ValidCost 2.984764575958252 | Accuracy : 0.558080792427063 - #\n",
      "1 Iteration Done.\n",
      " # - EPOCHS 1 / 10 | AvgCost 3496.81396484375 | Accuracy : 0.3248640298843384 - #\n",
      " # - EPOCHS 2 / 10 | AvgCost 1077.0758056640625 | Accuracy : 0.7612276673316956 - #\n",
      " # - EPOCHS 3 / 10 | AvgCost 759.26318359375 | Accuracy : 0.8518259525299072 - #\n",
      " # - EPOCHS 4 / 10 | AvgCost 632.4757690429688 | Accuracy : 0.8868687152862549 - #\n",
      " # - EPOCHS 5 / 10 | AvgCost 554.1742553710938 | Accuracy : 0.9027972221374512 - #\n",
      " # - EPOCHS 6 / 10 | AvgCost 513.8721923828125 | Accuracy : 0.9186480045318604 - #\n",
      " # - EPOCHS 7 / 10 | AvgCost 489.72149658203125 | Accuracy : 0.9249417185783386 - #\n",
      " # - EPOCHS 8 / 10 | AvgCost 440.85882568359375 | Accuracy : 0.938616931438446 - #\n",
      " # - EPOCHS 9 / 10 | AvgCost 434.8747863769531 | Accuracy : 0.9404817223548889 - #\n",
      " # - EPOCHS 10 / 10 | AvgCost 418.8481750488281 | Accuracy : 0.942890465259552 - #\n",
      " # - ValidCost 5.5976057052612305 | Accuracy : 0.401683509349823 - #\n",
      "2 Iteration Done.\n",
      " # - EPOCHS 1 / 10 | AvgCost 5652.6796875 | Accuracy : 0.04063713923096657 - #\n",
      " # - EPOCHS 2 / 10 | AvgCost 2289.333984375 | Accuracy : 0.31204351782798767 - #\n",
      " # - EPOCHS 3 / 10 | AvgCost 1493.8853759765625 | Accuracy : 0.5648018717765808 - #\n",
      " # - EPOCHS 4 / 10 | AvgCost 1187.1923828125 | Accuracy : 0.6829059720039368 - #\n",
      " # - EPOCHS 5 / 10 | AvgCost 990.3728637695312 | Accuracy : 0.7635586857795715 - #\n",
      " # - EPOCHS 6 / 10 | AvgCost 837.3021850585938 | Accuracy : 0.8318570256233215 - #\n",
      " # - EPOCHS 7 / 10 | AvgCost 733.5328979492188 | Accuracy : 0.8735819458961487 - #\n",
      " # - EPOCHS 8 / 10 | AvgCost 667.8381958007812 | Accuracy : 0.8980575203895569 - #\n",
      " # - EPOCHS 9 / 10 | AvgCost 596.1724243164062 | Accuracy : 0.91740483045578 - #\n",
      " # - EPOCHS 10 / 10 | AvgCost 581.7322387695312 | Accuracy : 0.9229215383529663 - #\n",
      " # - ValidCost 15.341231346130371 | Accuracy : 0.2503787875175476 - #\n",
      "3 Iteration Done.\n",
      " # - EPOCHS 1 / 10 | AvgCost 12011.8076171875 | Accuracy : 0.0 - #\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"  \n",
    "--div_data  : split your data or not.   \n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument('--div_data',   default = False)  # Change this with 'False' after dividing your datsaet into 10 groups.\n",
    "args = parser.parse_args(args=[])  \n",
    "\n",
    "# TODO : Saving tranined model in this location. Don't change this path. \n",
    "save_dir = './result'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# TODO : Seed\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# TODO : Split dataset according to argument '--div_data'\n",
    "if args.div_data == True:\n",
    "    train_split(validation_num)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "1. training      : train each 100 classes sequentailly with respect to 1000 output class. \n",
    "    trainining class === 1-100 -> 101-200 -> 201-300 -> 301-400 -> ... -> 901-1000\n",
    "    \n",
    "2. validation    : validate each trained model.\n",
    "    validation class === 1-100 -> 1-200 -> 1-300 -> ... -> 1-1000\n",
    "    \n",
    "3. model save    : saves each trained model.                \n",
    "\"\"\"\n",
    "\n",
    "for div_idx in range(10):\n",
    "\n",
    "    # TODO : Load your train and validation data\n",
    "    x_train, y_train = load_train_data(div_idx)\n",
    "    x_valid, y_valid = load_valid_data(div_idx)\n",
    "\n",
    "    \"\"\"\n",
    "        in case of tranining 1  -100 classes, validate on 1-100 classes\n",
    "        in case of tranining 101-200 classes, validate on 1-200 classes\n",
    "        in case of tranining 201-300 classes, validate on 1-300 classes\n",
    "        and so on...            \n",
    "    \"\"\"\n",
    "    \n",
    "    if div_idx == 0:\n",
    "        x_val_tmp = x_valid\n",
    "        y_val_tmp = y_valid\n",
    "    else:\n",
    "        x_val_tmp = np.concatenate((x_val_tmp, x_valid), axis = 0)\n",
    "        y_val_tmp = np.concatenate((y_val_tmp, y_valid), axis = 0)\n",
    "        x_valid   = x_val_tmp\n",
    "        y_valid   = y_val_tmp\n",
    "\n",
    "    # TODO : let the label starts from 0 to match the output index of model prediction. (Currently the label starts from 1.)\n",
    "    y_train = y_train - 1\n",
    "    y_valid = y_valid - 1\n",
    "\n",
    "    # TODO : Define dataset and dataloader\n",
    "    train_dataset     = CustomDataset(x_train, y_train, device)\n",
    "    valid_dataset     = CustomDataset(x_valid, y_valid, device)\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_data_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # TODO : train and validate\n",
    "    trained_model, acc_train = train_model(ewc, x_train, optimizer, num_epochs, train_data_loader, criterion,train_dataset,div_idx)\n",
    "    acc_valid                = validation(trained_model, x_valid, valid_data_loader, criterion)\n",
    "\n",
    "    #if div_idx == 9:\n",
    "    MODEL_SAVE_FOLDER_PATH = './model_save/'\n",
    "    num=str(div_idx)\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)        \n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + 'continual_model_ewc'+num+'.pt'\n",
    "    # TODO : save trained model in 'save_model_path'\n",
    "    torch.save(trained_model.state_dict(), model_path)\n",
    "\n",
    "    print(f'{str(div_idx)} Iteration Done.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
