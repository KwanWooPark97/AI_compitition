{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Guidelines of Sample Code (Pytorch)\n",
    "\n",
    "    See the annotations at every markdown blocks correspoding to each code blocks, and also # TODO annotations. :D\n",
    "\n",
    "# Usage guideline of Jupyter Notebook (If needed)\n",
    "\n",
    "    Installation   : https://jupyter.org/install  \n",
    "    User Document  : https://jupyter-notebook.readthedocs.io/en/latest/user-documentation.html\n",
    "\n",
    "# Test Environment (Recommended)\n",
    "\n",
    "    In test time, we will evaluate the given codes from you with the following version of libraries.  \n",
    "    So, it is highly recommended to use those packages with specific version below.\n",
    "\n",
    "    test environment : pytorch\n",
    "\n",
    "### Packages\n",
    "    python   : 3.8.17  \n",
    "    torch    : 2.0.1   \n",
    "    skimage  : 0.21.0  \n",
    "    cv2      : 4.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries (Do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset (Do not change!)\n",
    "\n",
    "### Notice 1\n",
    "    This function do split your dataset of 1000 classes into 10 groups of 100 each.    \n",
    "    So, it is needed to be implemented just once at first to split your dataset for continual learning.   \n",
    "    *Again, you dont need to use this function in every tranining time if you already split your dataset into 10 groups.\n",
    "\n",
    "    Notice the annotation codes below. (You can see this codes in 'main' block.)\n",
    "\n",
    "```python\n",
    "        parser = argparse.ArgumentParser()   \n",
    "        # Change this as 'False' after dividing your datsaet into 10 groups.\n",
    "        parser.add_argument('--div_data',   default = True)  \n",
    "        args = parser.parse_args(args=[])  \n",
    "```\n",
    "\n",
    "### Notice 2\n",
    "    We reshapes all the input data size into constant 128x128.   \n",
    "    Until further notification, use this constant size. \n",
    "\n",
    "```python\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 130):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))  # resize image into 128 x 128 \n",
    "                x_train.append(img)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(validation_num):\n",
    "    # TODO : set dataset path\n",
    "    # TODO : We recommends you to place your code and tranining dataset in the same location.\n",
    "    \n",
    "    dir = './Koh_Young_AI_data/'\n",
    "    \n",
    "\n",
    "    for div_idx in range(0, 10): # Div into 10 groups\n",
    "        # Divide data 0-129 for training, 130-150 for validation.\n",
    "        x_train = []\n",
    "        x_valid = []\n",
    "        y_train = []\n",
    "        y_valid = []\n",
    "        start   = 100*div_idx + 1\n",
    "        end     = 100*div_idx + 101\n",
    "\n",
    "        # Split input data.  \n",
    "        for i in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_train.append(img)\n",
    "\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                path = os.path.join(dir, str(i))\n",
    "                path = path + '/' +str(img_idx)+'.png'\n",
    "                img = io.imread(path)\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                x_valid.append(img)\n",
    "\n",
    "        # Split corresponding output label data.\n",
    "        for folder_idx in range(start, end):\n",
    "            for img_idx in range(0, 150-validation_num):\n",
    "                y_train.append(np.array([folder_idx]))\n",
    "            for img_idx in range(150-validation_num, 150):\n",
    "                y_valid.append(np.array([folder_idx]))\n",
    "\n",
    "        # Convert list to numpy \n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_valid = np.array(x_valid)\n",
    "        y_valid = np.array(y_valid)\n",
    "\n",
    "        # TODO : Define train data and valid data directory path.\n",
    "        # TODO : Recommends not to change these directory paths. \n",
    "        train_save_dir = 'train_data'\n",
    "        valid_save_dir = 'valid_data'\n",
    "        if not os.path.exists(train_save_dir):\n",
    "            os.makedirs(train_save_dir)\n",
    "\n",
    "        if not os.path.exists(valid_save_dir):\n",
    "            os.makedirs(valid_save_dir)\n",
    "\n",
    "        # TODO : Save train/valid data\n",
    "        np.save(f'./train_data/x_data_{div_idx+1}', x_train)\n",
    "        np.save(f'./train_data/y_data_{div_idx+1}', y_train)\n",
    "        np.save(f'./valid_data/x_data_{div_idx+1}', x_valid)\n",
    "        np.save(f'./valid_data/y_data_{div_idx+1}', y_valid)\n",
    "\n",
    "        print(f\" ===================== Done in {div_idx} ===================== \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataloader (Do not change!)\n",
    "\n",
    "    You can define your own dataloader with API of torch.utils.data.Dataset.  \n",
    "    This can usually help you to reduce computational burden when dealing with high dimensional data, such as images.  \n",
    "\n",
    "    reference url : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_data, y_data, device):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # .transpose(0, 2) : width x height x channel (0, 1, 2) ---> channel x width x height (2, 0, 1).\n",
    "        # .squeeze(0) : add extra dimension at axis 0.\n",
    "        x = torch.FloatTensor(self.x_data[idx]).transpose(0, 2)\n",
    "        y = torch.LongTensor(self.y_data[idx]).squeeze(0)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "def load_train_data(class_num):\n",
    "    # TODO : set 'class_path' with your train_data path.\n",
    "    class_path  = f'./train_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "    return x_data, y_data\n",
    "\n",
    "def load_valid_data(class_num):\n",
    "    # TODO : set 'class_path' with your valid_data path.\n",
    "    class_path  = f'./valid_data/'\n",
    "    x_data_path = class_path + 'x_data_' + str(class_num+1) + '.npy'\n",
    "    y_data_path = class_path + 'y_data_' + str(class_num+1) + '.npy'\n",
    "    x_data      = np.load(x_data_path, allow_pickle=True)\n",
    "    y_data      = np.load(y_data_path, allow_pickle=True)\n",
    "\n",
    "    # return processed data. \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tranining function (You can modify this part!)\n",
    "\n",
    "    Set your model with train mode as 'model.train()'.   \n",
    "\n",
    "    useful reference : https://wikidocs.net/195118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, optimizer, num_epochs, train_data_loader, criterion,train_dataset,indexs):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_train           : input data for tranining\n",
    "    optimizer         : optimizer\n",
    "    num_epoches       : number of iteration\n",
    "    train_data_loader : dataloder of training dataset\n",
    "    indexs            : train seq\n",
    "    \"\"\"\n",
    "    \n",
    "    if indexs== 0:\n",
    "        num_epochs=100\n",
    "    else:\n",
    "        num_epochs=50\n",
    "        \n",
    "    if indexs>0:\n",
    "        num_classess=(indexs+1)*100\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        #pretrained_dict = ewc.model.state_dict()\n",
    "        pretrained_dict = model.state_dict()\n",
    "        '''new_ewc = ElasticWeightConsolidation(ViT(\n",
    "            image_size = 128,\n",
    "            patch_size = 16,\n",
    "            num_classes = num_classess,\n",
    "            dim = 64,\n",
    "            depth = 6,\n",
    "            heads = 6,\n",
    "            mlp_dim = 128,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        ).to(device), crit=criterion, lr=2e-4)'''\n",
    "        new_model=ViT(\n",
    "            image_size = 128,\n",
    "            patch_size = 16,\n",
    "            num_classes = num_classess,\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 6,\n",
    "            mlp_dim = 256,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        ).to(device)\n",
    "        #new_ewc.register_ewc_params(train_dataset, 16, 100)\n",
    "        #new_model_dict = new_ewc.model.state_dict()\n",
    "        new_model_dict = new_model.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in new_model_dict}\n",
    "        pretrained_dict.pop('mlp_head.0.weight')\n",
    "        pretrained_dict.pop('mlp_head.0.bias')\n",
    "        pretrained_dict.pop('mlp_head.1.weight')\n",
    "        pretrained_dict.pop('mlp_head.1.bias')\n",
    "        #new_ewc.model.update(pretrained_dict)\n",
    "        new_model.load_state_dict(pretrained_dict,strict=False)\n",
    "        del model\n",
    "        #old_model=model\n",
    "        model=new_model\n",
    "    #ewc.model.train()                     # Set train mode. \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        acc      = 0 # Accuracy\n",
    "        avg_cost = 0 # Average Cost \n",
    "        for x, y in train_data_loader:\n",
    "            #loss=ewc.forward_backward_update(x.to(device), y.to(device))\n",
    "            #out = ewc.model(x.to(device))            # Inference (batch,classes)\n",
    "            out = model(x.to(device))\n",
    "            _, preds = torch.max(out, 1)         # preds : Predicted class\n",
    "            cost = criterion(out, y.to(device))  # Calculates errors between true label and predictions with respect to your criterion. \n",
    "\n",
    "            # Optimize processs.\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += cost # Average cost \n",
    "            acc      += torch.sum(preds.detach().cpu() == (y.data).detach().cpu()) # Accuracy\n",
    "        print(f\" # - EPOCHS {epoch + 1} / {num_epochs} | AvgCost {avg_cost} | Accuracy : {acc/len(x_train)} - #\")\n",
    "    #ewc.register_ewc_params(train_dataset, 16, 100)\n",
    "    #pretrained_dict = ewc.model.state_dict()\n",
    "    #print(pretrained_dict)\n",
    "    #exit(1)\n",
    "    # Return trainded model and accuracy. \n",
    "    return model, acc/len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define validataion function (Do not change!)\n",
    "\n",
    "    And eval mode as 'model.eval()' or 'model.train(False)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, x_valid, valid_data_loader, criterion):\n",
    "    \"\"\"\n",
    "    model             : your customized model \n",
    "    x_vallid          : input data for validation\n",
    "    valid_data_loader : dataloder of valid dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set eval mode\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    for x, y in valid_data_loader:\n",
    "        out = model(x.data.to(device))\n",
    "        _, preds = torch.max(out, 1)\n",
    "        cost  = criterion(out, y.to(device))\n",
    "        acc += torch.sum(preds.detach().cpu() == (y.data).detach().cpu())\n",
    "    print(f\" # - ValidCost {cost} | Accuracy : {acc / len(x_valid)} - #\")\n",
    "\n",
    "    # Return Accuracy \n",
    "    return acc/len(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model and hyperparameter (You can modify this part!)\n",
    "\n",
    "    Here is the pivotal part of your competition.\n",
    "    We gives a simple CNN model, for example. \n",
    "    Go make your own model!         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticWeightConsolidation:\n",
    "\n",
    "    def __init__(self, model, crit, lr=0.001, weight=1000000):\n",
    "        self.model = model\n",
    "        self.weight = weight\n",
    "        self.crit = crit\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "        #self.make_fisher()\n",
    "        \n",
    "    def _update_mean_params(self):\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            _buff_param_name = param_name.replace('.', '__')\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())\n",
    "\n",
    "    def _update_fisher_params(self, current_ds, batch_size, num_batch):\n",
    "        dl = DataLoader(current_ds, batch_size, shuffle=True)\n",
    "        log_liklihoods = []\n",
    "        for i, (inputs, target) in enumerate(dl):\n",
    "            if i > num_batch:\n",
    "                break\n",
    "            output = torch.nn.functional.log_softmax(self.model(inputs.to(device)), dim=1)\n",
    "            log_liklihoods.append(output[:, target])\n",
    "        log_likelihood = torch.cat(log_liklihoods).mean()\n",
    "        grad_log_liklihood = torch.autograd.grad(log_likelihood, self.model.parameters())\n",
    "        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "        for _buff_param_name, param in zip(_buff_param_names, grad_log_liklihood):\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_fisher', param.data.clone() ** 2)\n",
    "    def make_fisher(self,):\n",
    "        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "        for _buff_param_name in _buff_param_names:\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_fisher', torch.tensor(0))\n",
    "        for param_name, param in self.model.named_parameters():\n",
    "            _buff_param_name = param_name.replace('.', '__')\n",
    "            self.model.register_buffer(_buff_param_name+'_estimated_mean', torch.tensor(0))\n",
    "            \n",
    "    def register_ewc_params(self, dataset, batch_size, num_batches):\n",
    "        self._update_fisher_params(dataset, batch_size, num_batches)\n",
    "        self._update_mean_params()\n",
    "\n",
    "    def _compute_consolidation_loss(self, weight):\n",
    "        try:\n",
    "            losses = []\n",
    "            for param_name, param in self.model.named_parameters():\n",
    "                _buff_param_name = param_name.replace('.', '__')\n",
    "                estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name))\n",
    "                estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name))\n",
    "                losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())\n",
    "            return (weight / 2) * sum(losses)\n",
    "        except AttributeError:\n",
    "            return 0\n",
    "\n",
    "    def forward_backward_update(self, inputs, target):\n",
    "        output = self.model(inputs)\n",
    "        loss = self._compute_consolidation_loss(self.weight) + self.crit(output, target.to(device))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.model, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LSA(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads #64*16\n",
    "\n",
    "        self.heads = heads\n",
    "        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False) #output이 그럼 1024*3\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size=x.shape[0]\n",
    "        patch_size=x.shape[1]\n",
    "        x = self.norm(x)\n",
    "        #print(x.shape)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1) #to_qkv 하면 batch,patch,1024*3이 나옴 patch=257임\n",
    "        #test=torch.chunk(qkv,3,dim=-1)\n",
    "\n",
    "        #print(test)\n",
    "        q, k, v = map(lambda t: torch.reshape(t, (batch_size,self.heads,patch_size,-1)), qkv)\n",
    "        #q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv) #batch, head, patch, 나머지\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n",
    "\n",
    "        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "        dots = dots.masked_fill(mask, mask_value)\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = torch.reshape(out, (batch_size,patch_size,-1))\n",
    "        #out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class SPT(nn.Module):\n",
    "    def __init__(self, *, dim, patch_size, channels = 3):\n",
    "        super().__init__()\n",
    "        self.patch_dim = patch_size * patch_size * 5 * channels\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            #Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),#h=256//16\n",
    "            nn.LayerNorm(self.patch_dim),\n",
    "            nn.Linear(self.patch_dim, dim)\n",
    "        )#입력(batch,원래3인데 12개 추가해서 15, 256,256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size=x.shape[0]\n",
    "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
    "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
    "\n",
    "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
    "        #print(np.shape(torch.asarray(x_with_shifts).detach().numpy()))\n",
    "        x_with_shifts=torch.reshape(x_with_shifts,(batch_size,-1,self.patch_dim))\n",
    "        return self.to_patch_tokens(x_with_shifts)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        #print(x.shape)\n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        #cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        #print(np.shape(torch.asarray(x).detach().numpy()))\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        #exit(1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channel, num_class):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(64*16*16, self.num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # rescaling\n",
    "        x = x/255.0\n",
    "\n",
    "        # TODO : Convolution layer\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        # TODO : Reshape for fully-connected layer\n",
    "        out = out.view(-1, 64*16*16)\n",
    "\n",
    "        # TODO : Fully-connected layer\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        # TODO : final-pycharm output - 1000 (class num)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ewc = ElasticWeightConsolidation(ViT(\\n    image_size = 128,\\n    patch_size = 16,\\n    num_classes = 100,\\n    dim = 64,\\n    depth = 6,\\n    heads = 6,\\n    mlp_dim = 128,\\n    dropout = 0.1,\\n    emb_dropout = 0.1\\n).to(device), crit=criterion, lr=1e-4)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# TODO : Define your model\n",
    "\n",
    "model=ViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 6,\n",
    "    mlp_dim = 256,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "# TODO : Set your hyperparameters\n",
    "batch_size        = 16\n",
    "learning_rate     = 0.0001\n",
    "num_epochs        = 15\n",
    "optimizer         = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "random_seed       = 555\n",
    "validation_num    = 20 # for 150 images for class, the number for validation data\n",
    "criterion = nn.CrossEntropyLoss() # Define criterion. \n",
    "\n",
    "'''ewc = ElasticWeightConsolidation(ViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 64,\n",
    "    depth = 6,\n",
    "    heads = 6,\n",
    "    mlp_dim = 128,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device), crit=criterion, lr=1e-4)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning. (Do not change!)\n",
    "\n",
    "### WARNING:\n",
    "    The training and validation datasets each SHOULD BE prepared properly beforehand.  \n",
    "    If not, the submitted code from you will be immediately rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # - EPOCHS 1 / 100 | AvgCost 3643.383056640625 | Accuracy : 0.09300699084997177 - #\n",
      " # - EPOCHS 2 / 100 | AvgCost 3581.782958984375 | Accuracy : 0.16993007063865662 - #\n",
      " # - EPOCHS 3 / 100 | AvgCost 3528.75390625 | Accuracy : 0.24009324610233307 - #\n",
      " # - EPOCHS 4 / 100 | AvgCost 3472.34326171875 | Accuracy : 0.31351980566978455 - #\n",
      " # - EPOCHS 5 / 100 | AvgCost 3414.3408203125 | Accuracy : 0.3852369785308838 - #\n",
      " # - EPOCHS 6 / 100 | AvgCost 3389.180908203125 | Accuracy : 0.41546231508255005 - #\n",
      " # - EPOCHS 7 / 100 | AvgCost 3364.094482421875 | Accuracy : 0.44630923867225647 - #\n",
      " # - EPOCHS 8 / 100 | AvgCost 3349.8515625 | Accuracy : 0.46309247612953186 - #\n",
      " # - EPOCHS 9 / 100 | AvgCost 3326.056396484375 | Accuracy : 0.49494948983192444 - #\n",
      " # - EPOCHS 10 / 100 | AvgCost 3297.87353515625 | Accuracy : 0.5305361151695251 - #\n",
      " # - EPOCHS 11 / 100 | AvgCost 3244.88916015625 | Accuracy : 0.6010878086090088 - #\n",
      " # - EPOCHS 12 / 100 | AvgCost 3202.164306640625 | Accuracy : 0.653224527835846 - #\n",
      " # - EPOCHS 13 / 100 | AvgCost 3168.9248046875 | Accuracy : 0.6936286091804504 - #\n",
      " # - EPOCHS 14 / 100 | AvgCost 3142.815673828125 | Accuracy : 0.7257187366485596 - #\n",
      " # - EPOCHS 15 / 100 | AvgCost 3130.19873046875 | Accuracy : 0.7392385601997375 - #\n",
      " # - EPOCHS 16 / 100 | AvgCost 3116.64599609375 | Accuracy : 0.757886528968811 - #\n",
      " # - EPOCHS 17 / 100 | AvgCost 3108.40478515625 | Accuracy : 0.7651903629302979 - #\n",
      " # - EPOCHS 18 / 100 | AvgCost 3096.989013671875 | Accuracy : 0.7804972529411316 - #\n",
      " # - EPOCHS 19 / 100 | AvgCost 3087.422119140625 | Accuracy : 0.7905982732772827 - #\n",
      " # - EPOCHS 20 / 100 | AvgCost 3070.371337890625 | Accuracy : 0.8129758834838867 - #\n",
      " # - EPOCHS 21 / 100 | AvgCost 3050.33740234375 | Accuracy : 0.8412587642669678 - #\n",
      " # - EPOCHS 22 / 100 | AvgCost 3030.310546875 | Accuracy : 0.8661227822303772 - #\n",
      " # - EPOCHS 23 / 100 | AvgCost 3011.47265625 | Accuracy : 0.8889665603637695 - #\n",
      " # - EPOCHS 24 / 100 | AvgCost 2998.064208984375 | Accuracy : 0.906216025352478 - #\n",
      " # - EPOCHS 25 / 100 | AvgCost 2989.1416015625 | Accuracy : 0.9138306379318237 - #\n",
      " # - EPOCHS 26 / 100 | AvgCost 2982.940673828125 | Accuracy : 0.9202797412872314 - #\n",
      " # - EPOCHS 27 / 100 | AvgCost 2980.056640625 | Accuracy : 0.9222999215126038 - #\n",
      " # - EPOCHS 28 / 100 | AvgCost 2976.2978515625 | Accuracy : 0.9278943538665771 - #\n",
      " # - EPOCHS 29 / 100 | AvgCost 2970.27197265625 | Accuracy : 0.9347319602966309 - #\n",
      " # - EPOCHS 30 / 100 | AvgCost 2968.96923828125 | Accuracy : 0.9361305236816406 - #\n",
      " # - EPOCHS 31 / 100 | AvgCost 2967.08837890625 | Accuracy : 0.9373737573623657 - #\n",
      " # - EPOCHS 32 / 100 | AvgCost 2961.810302734375 | Accuracy : 0.9435897469520569 - #\n",
      " # - EPOCHS 33 / 100 | AvgCost 2959.533935546875 | Accuracy : 0.9463092684745789 - #\n",
      " # - EPOCHS 34 / 100 | AvgCost 2958.60693359375 | Accuracy : 0.9476301670074463 - #\n",
      " # - EPOCHS 35 / 100 | AvgCost 2954.531494140625 | Accuracy : 0.95252525806427 - #\n",
      " # - EPOCHS 36 / 100 | AvgCost 2952.89697265625 | Accuracy : 0.9542346596717834 - #\n",
      " # - EPOCHS 37 / 100 | AvgCost 2952.835693359375 | Accuracy : 0.953690767288208 - #\n",
      " # - EPOCHS 38 / 100 | AvgCost 2952.23291015625 | Accuracy : 0.9540015459060669 - #\n",
      " # - EPOCHS 39 / 100 | AvgCost 2953.046630859375 | Accuracy : 0.9531468749046326 - #\n",
      " # - EPOCHS 40 / 100 | AvgCost 2950.687255859375 | Accuracy : 0.9562548398971558 - #\n",
      " # - EPOCHS 41 / 100 | AvgCost 2947.34033203125 | Accuracy : 0.961383044719696 - #\n",
      " # - EPOCHS 42 / 100 | AvgCost 2941.7490234375 | Accuracy : 0.967599093914032 - #\n",
      " # - EPOCHS 43 / 100 | AvgCost 2937.705078125 | Accuracy : 0.9710955619812012 - #\n",
      " # - EPOCHS 44 / 100 | AvgCost 2934.665283203125 | Accuracy : 0.9750582575798035 - #\n",
      " # - EPOCHS 45 / 100 | AvgCost 2935.7587890625 | Accuracy : 0.9746697545051575 - #\n",
      " # - EPOCHS 46 / 100 | AvgCost 2937.5830078125 | Accuracy : 0.971794843673706 - #\n",
      " # - EPOCHS 47 / 100 | AvgCost 2934.489013671875 | Accuracy : 0.9756021499633789 - #\n",
      " # - EPOCHS 48 / 100 | AvgCost 2933.487548828125 | Accuracy : 0.9763791561126709 - #\n",
      " # - EPOCHS 49 / 100 | AvgCost 2933.015625 | Accuracy : 0.976456880569458 - #\n",
      " # - EPOCHS 50 / 100 | AvgCost 2932.780029296875 | Accuracy : 0.9771561622619629 - #\n",
      " # - EPOCHS 51 / 100 | AvgCost 2934.446044921875 | Accuracy : 0.9743589758872986 - #\n",
      " # - EPOCHS 52 / 100 | AvgCost 2932.333740234375 | Accuracy : 0.9773892760276794 - #\n",
      " # - EPOCHS 53 / 100 | AvgCost 2932.449462890625 | Accuracy : 0.97723388671875 - #\n",
      " # - EPOCHS 54 / 100 | AvgCost 2932.06103515625 | Accuracy : 0.9777000546455383 - #\n",
      " # - EPOCHS 55 / 100 | AvgCost 2930.416015625 | Accuracy : 0.9801864624023438 - #\n",
      " # - EPOCHS 56 / 100 | AvgCost 2928.873046875 | Accuracy : 0.9811188578605652 - #\n",
      " # - EPOCHS 57 / 100 | AvgCost 2931.046875 | Accuracy : 0.978787899017334 - #\n",
      " # - EPOCHS 58 / 100 | AvgCost 2931.277587890625 | Accuracy : 0.978399395942688 - #\n",
      " # - EPOCHS 59 / 100 | AvgCost 2930.668701171875 | Accuracy : 0.9789432883262634 - #\n",
      " # - EPOCHS 60 / 100 | AvgCost 2930.026611328125 | Accuracy : 0.9796425700187683 - #\n",
      " # - EPOCHS 61 / 100 | AvgCost 2928.743408203125 | Accuracy : 0.9812743067741394 - #\n",
      " # - EPOCHS 62 / 100 | AvgCost 2929.404296875 | Accuracy : 0.9800310730934143 - #\n",
      " # - EPOCHS 63 / 100 | AvgCost 2930.20654296875 | Accuracy : 0.9796425700187683 - #\n",
      " # - EPOCHS 64 / 100 | AvgCost 2930.060546875 | Accuracy : 0.9793317914009094 - #\n",
      " # - EPOCHS 65 / 100 | AvgCost 2928.465087890625 | Accuracy : 0.9812743067741394 - #\n",
      " # - EPOCHS 66 / 100 | AvgCost 2928.205322265625 | Accuracy : 0.9817404747009277 - #\n",
      " # - EPOCHS 67 / 100 | AvgCost 2927.21728515625 | Accuracy : 0.9828282594680786 - #\n",
      " # - EPOCHS 68 / 100 | AvgCost 2928.08349609375 | Accuracy : 0.9817404747009277 - #\n",
      " # - EPOCHS 69 / 100 | AvgCost 2927.523681640625 | Accuracy : 0.9827505946159363 - #\n",
      " # - EPOCHS 70 / 100 | AvgCost 2928.410888671875 | Accuracy : 0.9814296960830688 - #\n",
      " # - EPOCHS 71 / 100 | AvgCost 2928.79833984375 | Accuracy : 0.9811965823173523 - #\n",
      " # - EPOCHS 72 / 100 | AvgCost 2928.91064453125 | Accuracy : 0.9805749654769897 - #\n",
      " # - EPOCHS 73 / 100 | AvgCost 2928.87109375 | Accuracy : 0.9803418517112732 - #\n",
      " # - EPOCHS 74 / 100 | AvgCost 2926.781982421875 | Accuracy : 0.9829059839248657 - #\n",
      " # - EPOCHS 75 / 100 | AvgCost 2927.936767578125 | Accuracy : 0.9817404747009277 - #\n",
      " # - EPOCHS 76 / 100 | AvgCost 2928.904052734375 | Accuracy : 0.9804973006248474 - #\n",
      " # - EPOCHS 77 / 100 | AvgCost 2927.404052734375 | Accuracy : 0.9822843670845032 - #\n",
      " # - EPOCHS 78 / 100 | AvgCost 2927.211181640625 | Accuracy : 0.9828282594680786 - #\n",
      " # - EPOCHS 79 / 100 | AvgCost 2926.995361328125 | Accuracy : 0.9831390976905823 - #\n",
      " # - EPOCHS 80 / 100 | AvgCost 2926.9267578125 | Accuracy : 0.9832944869995117 - #\n",
      " # - EPOCHS 81 / 100 | AvgCost 2927.396728515625 | Accuracy : 0.9818958640098572 - #\n",
      " # - EPOCHS 82 / 100 | AvgCost 2926.48828125 | Accuracy : 0.9832167625427246 - #\n",
      " # - EPOCHS 83 / 100 | AvgCost 2926.81884765625 | Accuracy : 0.9825952053070068 - #\n",
      " # - EPOCHS 84 / 100 | AvgCost 2928.01953125 | Accuracy : 0.9814296960830688 - #\n",
      " # - EPOCHS 85 / 100 | AvgCost 2927.88427734375 | Accuracy : 0.9813519716262817 - #\n",
      " # - EPOCHS 86 / 100 | AvgCost 2928.262451171875 | Accuracy : 0.9808858036994934 - #\n",
      " # - EPOCHS 87 / 100 | AvgCost 2927.59033203125 | Accuracy : 0.9819735884666443 - #\n",
      " # - EPOCHS 88 / 100 | AvgCost 2926.6640625 | Accuracy : 0.9832944869995117 - #\n",
      " # - EPOCHS 89 / 100 | AvgCost 2926.760986328125 | Accuracy : 0.9829059839248657 - #\n",
      " # - EPOCHS 90 / 100 | AvgCost 2926.38720703125 | Accuracy : 0.9835276007652283 - #\n",
      " # - EPOCHS 91 / 100 | AvgCost 2927.872802734375 | Accuracy : 0.9815850853919983 - #\n",
      " # - EPOCHS 92 / 100 | AvgCost 2926.25341796875 | Accuracy : 0.9836052656173706 - #\n",
      " # - EPOCHS 93 / 100 | AvgCost 2926.2890625 | Accuracy : 0.9839937686920166 - #\n",
      " # - EPOCHS 94 / 100 | AvgCost 2924.714111328125 | Accuracy : 0.9854700565338135 - #\n",
      " # - EPOCHS 95 / 100 | AvgCost 2926.488037109375 | Accuracy : 0.9831390976905823 - #\n",
      " # - EPOCHS 96 / 100 | AvgCost 2926.9912109375 | Accuracy : 0.9825952053070068 - #\n",
      " # - EPOCHS 97 / 100 | AvgCost 2925.728515625 | Accuracy : 0.9838383793830872 - #\n",
      " # - EPOCHS 98 / 100 | AvgCost 2926.568359375 | Accuracy : 0.9828282594680786 - #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # - EPOCHS 99 / 100 | AvgCost 2927.109375 | Accuracy : 0.9828282594680786 - #\n",
      " # - EPOCHS 100 / 100 | AvgCost 2925.700439453125 | Accuracy : 0.9840714931488037 - #\n",
      " # - ValidCost 3.6591711044311523 | Accuracy : 0.8868687152862549 - #\n",
      "0 Iteration Done.\n",
      " # - EPOCHS 1 / 50 | AvgCost 4259.7578125 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 2 / 50 | AvgCost 4259.76171875 | Accuracy : 0.00901320856064558 - #\n",
      " # - EPOCHS 3 / 50 | AvgCost 4259.76806640625 | Accuracy : 0.007692307699471712 - #\n",
      " # - EPOCHS 4 / 50 | AvgCost 4259.75341796875 | Accuracy : 0.008158507756888866 - #\n",
      " # - EPOCHS 5 / 50 | AvgCost 4259.77197265625 | Accuracy : 0.008780108764767647 - #\n",
      " # - EPOCHS 6 / 50 | AvgCost 4259.7509765625 | Accuracy : 0.009401709772646427 - #\n",
      " # - EPOCHS 7 / 50 | AvgCost 4259.7724609375 | Accuracy : 0.008236207999289036 - #\n",
      " # - EPOCHS 8 / 50 | AvgCost 4259.7783203125 | Accuracy : 0.009479409083724022 - #\n",
      " # - EPOCHS 9 / 50 | AvgCost 4259.74365234375 | Accuracy : 0.008935509249567986 - #\n",
      " # - EPOCHS 10 / 50 | AvgCost 4259.79150390625 | Accuracy : 0.007692307699471712 - #\n",
      " # - EPOCHS 11 / 50 | AvgCost 4259.7646484375 | Accuracy : 0.008313908241689205 - #\n",
      " # - EPOCHS 12 / 50 | AvgCost 4259.7939453125 | Accuracy : 0.008469308726489544 - #\n",
      " # - EPOCHS 13 / 50 | AvgCost 4259.7734375 | Accuracy : 0.008624708279967308 - #\n",
      " # - EPOCHS 14 / 50 | AvgCost 4259.75390625 | Accuracy : 0.009324009530246258 - #\n",
      " # - EPOCHS 15 / 50 | AvgCost 4259.7666015625 | Accuracy : 0.008158507756888866 - #\n",
      " # - EPOCHS 16 / 50 | AvgCost 4259.76806640625 | Accuracy : 0.008857809007167816 - #\n",
      " # - EPOCHS 17 / 50 | AvgCost 4259.755859375 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 18 / 50 | AvgCost 4259.78125 | Accuracy : 0.008158507756888866 - #\n",
      " # - EPOCHS 19 / 50 | AvgCost 4259.7568359375 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 20 / 50 | AvgCost 4259.79833984375 | Accuracy : 0.008313908241689205 - #\n",
      " # - EPOCHS 21 / 50 | AvgCost 4259.77734375 | Accuracy : 0.007925407961010933 - #\n",
      " # - EPOCHS 22 / 50 | AvgCost 4259.80224609375 | Accuracy : 0.006682206876575947 - #\n",
      " # - EPOCHS 23 / 50 | AvgCost 4259.7763671875 | Accuracy : 0.0072261071763932705 - #\n",
      " # - EPOCHS 24 / 50 | AvgCost 4259.78173828125 | Accuracy : 0.0077700079418718815 - #\n",
      " # - EPOCHS 25 / 50 | AvgCost 4259.7841796875 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 26 / 50 | AvgCost 4259.75 | Accuracy : 0.009246309287846088 - #\n",
      " # - EPOCHS 27 / 50 | AvgCost 4259.78271484375 | Accuracy : 0.007847707718610764 - #\n",
      " # - EPOCHS 28 / 50 | AvgCost 4259.76904296875 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 29 / 50 | AvgCost 4259.78369140625 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 30 / 50 | AvgCost 4259.78125 | Accuracy : 0.008080808445811272 - #\n",
      " # - EPOCHS 31 / 50 | AvgCost 4259.74755859375 | Accuracy : 0.008391608484089375 - #\n",
      " # - EPOCHS 32 / 50 | AvgCost 4259.79150390625 | Accuracy : 0.007847707718610764 - #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"  \n",
    "--div_data  : split your data or not.   \n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument('--div_data',   default = False)  # Change this with 'False' after dividing your datsaet into 10 groups.\n",
    "args = parser.parse_args(args=[])  \n",
    "\n",
    "# TODO : Saving tranined model in this location. Don't change this path. \n",
    "save_dir = './result'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# TODO : Seed\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# TODO : Split dataset according to argument '--div_data'\n",
    "if args.div_data == True:\n",
    "    train_split(validation_num)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "1. training      : train each 100 classes sequentailly with respect to 1000 output class. \n",
    "    trainining class === 1-100 -> 101-200 -> 201-300 -> 301-400 -> ... -> 901-1000\n",
    "    \n",
    "2. validation    : validate each trained model.\n",
    "    validation class === 1-100 -> 1-200 -> 1-300 -> ... -> 1-1000\n",
    "    \n",
    "3. model save    : saves each trained model.                \n",
    "\"\"\"\n",
    "\n",
    "for div_idx in range(10):\n",
    "\n",
    "    # TODO : Load your train and validation data\n",
    "    x_train, y_train = load_train_data(div_idx)\n",
    "    x_valid, y_valid = load_valid_data(div_idx)\n",
    "\n",
    "    \"\"\"\n",
    "        in case of tranining 1  -100 classes, validate on 1-100 classes\n",
    "        in case of tranining 101-200 classes, validate on 1-200 classes\n",
    "        in case of tranining 201-300 classes, validate on 1-300 classes\n",
    "        and so on...            \n",
    "    \"\"\"\n",
    "    \n",
    "    if div_idx == 0:\n",
    "        x_val_tmp = x_valid\n",
    "        y_val_tmp = y_valid\n",
    "    else:\n",
    "        x_val_tmp = np.concatenate((x_val_tmp, x_valid), axis = 0)\n",
    "        y_val_tmp = np.concatenate((y_val_tmp, y_valid), axis = 0)\n",
    "        x_valid   = x_val_tmp\n",
    "        y_valid   = y_val_tmp\n",
    "\n",
    "    # TODO : let the label starts from 0 to match the output index of model prediction. (Currently the label starts from 1.)\n",
    "    y_train = y_train - 1\n",
    "    y_valid = y_valid - 1\n",
    "\n",
    "    # TODO : Define dataset and dataloader\n",
    "    train_dataset     = CustomDataset(x_train, y_train, device)\n",
    "    valid_dataset     = CustomDataset(x_valid, y_valid, device)\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_data_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # TODO : train and validate\n",
    "    trained_model, acc_train = train_model(model, x_train, optimizer, num_epochs, train_data_loader, criterion,train_dataset,div_idx)\n",
    "    acc_valid                = validation(trained_model, x_valid, valid_data_loader, criterion)\n",
    "\n",
    "    #if div_idx == 9:\n",
    "    MODEL_SAVE_FOLDER_PATH = './model_save/'\n",
    "    num=str(div_idx)\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)        \n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + 'continual_model'+num+'.pt'\n",
    "    # TODO : save trained model in 'save_model_path'\n",
    "    torch.save(trained_model.state_dict(), model_path)\n",
    "\n",
    "    print(f'{str(div_idx)} Iteration Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "model_dict = model.state_dict()\n",
    "# remove the keys corresponing to the linear layer in the pretrained_dict\n",
    "#model_dict.pop(mlp_head.0.weight)\n",
    "#model_dict.pop(mlp_head.0.bias)\n",
    "#model_dict.pop(mlp_head.1.weight)\n",
    "#model_dict.pop(mlp_head.1.bias)\n",
    "# now update the model dict with pretrained dict\n",
    "print(model_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
