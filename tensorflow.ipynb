{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Guidelines of Sample Code (Tensorflow)\n",
    "\n",
    "    See the annotations at every markdown blocks correspoding to each code blocks, and also # TODO annotations. :D\n",
    "\n",
    "# Usage guideline of Jupyter Notebook (If needed)\n",
    "\n",
    "    Installation   : https://jupyter.org/install  \n",
    "    User Document  : https://jupyter-notebook.readthedocs.io/en/latest/user-documentation.html\n",
    "\n",
    "# Test Environment (Recommended)\n",
    "\n",
    "    In test time, we will evaluate the given codes from you with the following version of libraries.  \n",
    "    So, it is highly recommended to use those packages with specific version below.\n",
    "\n",
    "    test environment : tensorflow2\n",
    "\n",
    "### Packages\n",
    "    python      : 3.9.15  \n",
    "    tensorflow  : 2.8.0  \n",
    "    keras       : 2.8.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import python libraries (Do not change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0uY385UvNcX8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your model and hyperparameter (You can modify this part!)\n",
    "\n",
    "    Here is the pivotal part of your competition.\n",
    "    We gives a simple CNN model, for example. \n",
    "    Go make your own model! \n",
    "\n",
    "### Notice\n",
    "    We reshapes all the input data size into constant 128x128.  \n",
    "    Until further notification, use this constant size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Set your hyperparameters (Note that img_height and img_width have to be fixed)\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = 0.1\n",
    "random_seed = 123\n",
    "EPOCH = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# TODO : Define your own network!\n",
    "\n",
    "def basic_cnn(input_shape):\n",
    "    cnn = keras.models.Sequential([\n",
    "        layers.Rescaling(1./255.0),\n",
    "        layers.Conv2D(32, kernel_size=3, input_shape=input_shape, activation='relu'),\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPool2D(),   \n",
    "\n",
    "        layers.Conv2D(64, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1000),\n",
    "        layers.Softmax(),\n",
    "    ])\n",
    "    return cnn\n",
    "\n",
    "model = basic_cnn(input_shape=(128, 128, 3))\n",
    "\n",
    "# TODO : Define your own Optimizer!\n",
    "\n",
    "model.compile(tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load with tensorflow dataset library (Do not change!)\n",
    "\n",
    "    data_dir : 'YOUR DIR'  \n",
    "\n",
    "    You can define your own dataloader with API of tf.keras.utils.image_dataset_from_directory.  \n",
    "    This can usually help you to reduce computational burden when dealing with high dimensional data, such as images.  \n",
    "\n",
    "    reference url : https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150000 files belonging to 1000 classes.\n",
      "Using 135000 files for training.\n",
      "Found 150000 files belonging to 1000 classes.\n",
      "Using 15000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# TODO : set dataset path\n",
    "# TODO : We recommends you to place your code and tranining dataset in the same location.\n",
    "\n",
    "data_dir = './Koh_Young_AI_data/'\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=validation_split,\n",
    "  subset=\"training\",\n",
    "  seed=random_seed,\n",
    "  image_size=(128, 128),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=validation_split,\n",
    "  subset=\"validation\",\n",
    "  seed=random_seed,\n",
    "  image_size=(128, 128),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning. (You can modify this part!)\n",
    "\n",
    "### WARNING:\n",
    "    The training and validation datasets each SHOULD BE prepared properly beforehand.  \n",
    "    If not, the submitted code from you will be immediately rejected.\n",
    "\n",
    "### Notice\n",
    "    This function do split your dataset of 1000 classes into 10 groups of 100 each.    \n",
    "    So, it is needed to be implemented just once at first to split your dataset for continual learning.   \n",
    "    *Again, you dont need to use this function in every tranining time if you already split your dataset into 10 groups.\n",
    "\n",
    "    Notice the annotation codes below. (You can see this codes.)\n",
    "\n",
    "```python\n",
    "        train = train_ds.unbatch().filter(lambda x, y: tf.greater_equal(y, int(0 + 100 * iteration)))\n",
    "        train = train.filter(lambda x, y: tf.greater(int(100 + 100 * iteration), y)).batch(batch_size)\n",
    "        val = val_ds.unbatch().filter(lambda x, y: tf.greater(int(100 + 100 * iteration), y)).batch(batch_size)\n",
    "```\n",
    "### WARNING\n",
    "    The final submission is a weight file that can classify all 1000 classes.   \n",
    "    You can modify the code but be careful to properly submit the last weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m### Model training\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[39m### Data split for continual learning\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     train \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39munbatch()\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x, y: tf\u001b[39m.\u001b[39mgreater_equal(y, \u001b[39mint\u001b[39m(\u001b[39m0\u001b[39m \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m iteration)))\n\u001b[1;32m      6\u001b[0m     train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x, y: tf\u001b[39m.\u001b[39mgreater(\u001b[39mint\u001b[39m(\u001b[39m100\u001b[39m \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m iteration), y))\u001b[39m.\u001b[39mbatch(batch_size)\n\u001b[1;32m      7\u001b[0m     val \u001b[39m=\u001b[39m val_ds\u001b[39m.\u001b[39munbatch()\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x, y: tf\u001b[39m.\u001b[39mgreater(\u001b[39mint\u001b[39m(\u001b[39m100\u001b[39m \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m iteration), y))\u001b[39m.\u001b[39mbatch(batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "### Model training\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    ### Data split for continual learning\n",
    "    train = train_ds.unbatch().filter(lambda x, y: tf.greater_equal(y, int(0 + 100 * iteration)))\n",
    "    train = train.filter(lambda x, y: tf.greater(int(100 + 100 * iteration), y)).batch(batch_size)\n",
    "    val = val_ds.unbatch().filter(lambda x, y: tf.greater(int(100 + 100 * iteration), y)).batch(batch_size)\n",
    "\n",
    "    if iteration != 9:\n",
    "        model.fit(train, validation_data=val, epochs=EPOCH)\n",
    "    else:\n",
    "        MODEL_SAVE_FOLDER_PATH = './model_save/'\n",
    "        if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "            os.mkdir(MODEL_SAVE_FOLDER_PATH)        \n",
    "        model_path = MODEL_SAVE_FOLDER_PATH + 'continual_model.h5'\n",
    "        cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                                        verbose=1, save_best_only=True)\n",
    "        \n",
    "        model.fit(train, validation_data=val, epochs=EPOCH, callbacks=[cb_checkpoint])\n",
    "    print(f'{str(iteration+1)} Iteration Done.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
